<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Mémoire</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <section id="splash">
    <h1 id="titre">Derrière l'écran</h1>
  </section>

  <section id="table-of-content">
    <ul id="table-list">
      <li><a href="#intro">Introduction</a></li>
      <li><a href="#first">Origines de l’interface</a></li>
      <li><a href="#second">La simplicité se vend bien</a></li>
      <li><a href="#third">Dissimuler</a></li>
      <li><a href="#fourth">Déterminer</a></li>
      <li><a href="#fifth">Une sociéte scindée</a></li>
      <li><a href="#sixth">Pour conclure</a></li>
      <li><a href="#seventh">Bibliographie</a></li>

    </ul>
  </section>

  <section class="chapter" data-chapter="0">
    <h1 id="intro">Introduction</h1>
    <p>S’il y a bien un terme que l’industrie numérique revendique systématiquement, c’est celui-ci : simplicité.</p>
    <p>
      Alors qu’en 2007 Steve Jobs annonçait l’iPhone, un mot était récurent tout au long de sa présentation : simple. Ce
      terme deviendra le leitmotiv de la présentation de tout produit technologique pour grand public.<em> Simple.</em>
      Débarrassés des boutons grossiers qui occupaient les téléphones de l’époque, l’écran et ce qu’il montrait
      occupaient désormais une place capitale. La simplicité dont Steve Jobs chantait les louanges ne se manifestait pas
      seulement dans l’objet physique. La véritable simplicité semblait résider au-delà de la vitre, dans la manière
      dont on utilisait cet ordinateur miniature. L’interface graphique prenait le dessus.
    </p>


    <p>
      L’interface graphique, communément désignée par l’acronyme GUI <span class="margin-note">Graphical User
        Interface</span> est la frontière entre l’utilisateur et l’ordinateur, entre le code, et le logiciel (ou
      l’application) qu’il manipule. Elle se positionne comme médiatrice, transformant ces instructions cryptiques en
      formes standardisées, intelligibles par l’humain. Si Anthony Masure nous propose la définition suivante : </p>

    <p class="cite">« Une interface est un empilement de couches techniques dont les effets masquent le fonctionnement
      interne des machines »</p>

    <p>nous pouvons avancer que l’interface graphique est la strate supérieure de cet empilement de couches.</p>
    <p>

      Cette formulation semble impliquer que l’interface graphique devrait occulter la réalité technique des processus
      aux œuvres dans nos ordinateurs. Mais est-ce vraiment la condition <em>sine qua non</em> de l’existence d’une
      interface
      graphique ?
    </p>
    <p>Si l’iPhone est si important dans l’histoire de l’interface graphique, c’est parce qu’il a donné naissance à une
      nouvelle ère. Une ère où l’affordance des interfaces virtuelles est la préoccupation principale des designers
      d’interface. Il a ouvert le chemin à la prolifération d’interfaces graphiques accessibles, manipulables par le
      grand public, et ce, au prix d’une certaine perte de liberté de l’utilisateur. Cette motivation de rechercher la
      facilité d’utilisation comme finalité n’a pourtant pas été la raison de la naissance des interfaces graphiques.
    </p>
    <p>À la fois formidable solution pour manipuler un ordinateur avec peu de connaissances au préalable ou effectuer du
      travail graphique, l’interface graphique est aussi ce qui vient dissimuler le fonctionnement de la machine et
      limiter la connaissance de l’utilisateur, l’empêchant de s’émanciper pleinement par la technologie. Elle serait
      donc un pharmakon. En reprenant ce terme largement employé par Bernard Stiegler afin de qualifier les objets
      techniques, je cherche à souligner l’ambiguïté du rôle du GUI. </p>
    <p>Sous le fantasme d’un monde où la puissance des ordinateurs est accessible à un public plus vaste que des
      ingénieurs, les interfaces graphiques ont une puissance de contrôle qui n’est pas systématiquement perceptible. À
      l’heure où notre société tend à tout numériser (travail, loisir, communication, consommation), il est
      problématique d’imaginer la majeure partie du monde comme ayant une compréhension de la technologie qu’ils
      utilisent quotidiennement qui s’arrête à l’image que leur renvoient leurs écrans. Il convient alors de se poser la
      question suivante : peut-on accepter que la simplicité d’utilisation d’un ordinateur soit une fin en soi ?</p>

    <p>Nous chercherons à comprendre comment la recherche de la simplicité s’est imposée comme axiome au cours de
      l’histoire du design d’interface. Dans un second temps, nous émettrons une critique de cette philosophie.</p>
  </section>


  <section class="chapter" data-chapter="1">
    <h1 id="first">Origines de l’nterface</h1>

    <p>Augmentation Research Center, Stanford Research Institute, année 68. Douglas Engelbart et son équipe présentent
      une machine qui donnera forme à l’informatique pour les décennies à venir. Au cours de cet évènement dont on se
      rappellera plus tard sous le nom de <em>« the mother of all demo »</em> le oN-Line System, ou NLS est révélé.
      C’est le premier ordinateur employant une utilisation pratique de liens hypertextes, de la souris, de l’e-mail et
      bien d’autres principes modernes de l’informatique. L’utilisation d’un nouveau type d’écran à matrice de points
      permit même la représentation d’image vidéo, et leur diffusion en temps réel. Il fut surtout le premier ordinateur
      à être doté d’une interface graphique permettant la représentation et l’édition de documents en deux dimensions.
      Nous sommes bien loin des ordinateurs de cette époque, les super calculateurs, machines gigantesques avec
      lesquelles on communique généralement avec des cartes perforées, des kilomètres de câbles ou des étendues
      d’interrupteurs.</p>

    <p>Cette manière radicalement nouvelle d’interagir avec les ordinateurs ne résulte pas de la volonté de simplifier
      leur usage. En 1960, J.C.R. Licklider revendiquait la nécessité de développer une informatique sensible à des
      changements de variables, une informatique en temps réel dans un essai intitulé <em>« Man Computer
        Symbiosis »</em>. Cet écrit influença Engelbart : deux ans plus tard, il écrivait <em>« Augmenting Human
        Intellect »</em>, un essai soutenant que l’ordinateur avait le potentiel d’assister l’humain dans des tâches
      autres que le calcul mathématique.</p>

    <div class="img-in-text">
      <img src="./img/chapter1/NLSinterface.jpg" id="img_01" />
      <p class="description"><em>Vidéo-conférence sur le NLS, 1968.</em></p>
    </div>

    <p class="cite break-cite">
      « Dans une telle relation future de travail entre l’appareil à résoudre les problèmes humains et l’ordinateur
      agissant comme un valet, l’aptitude de l’ordinateur à exécuter des processus mathématiques serait utilisée pour
      être appliquée à une autre tâche au moment où on en aurait besoin. L’ordinateur présente bien d’autres aptitudes
      de manipulation et d’affichage de l’information, susceptibles de représenter un bénéfice significatif pour l’être
      humain dans des processus non mathématiques de planification, d’organisation, d’étude, etc. Toute personne qui
      organise sa pensée en utilisant des concepts symbolisés (que ce soit sous la forme de la langue anglaise, de
      pictogrammes, de logique formelle, ou de mathématiques) devrait être en mesure d’en bénéficier de manière
      significative. »
    </p>
    <p class="description">Douglas Engelbart - <em>Augmenting human intellect</em></p>


    <img src="./img/chapter1/BillEnglish1968-1.jpg" class="img-full-page img-double-page-left" id="img_02" />
    <img src="./img/chapter1/BillEnglish1968-2.jpg" class="img-full-page img-double-page-right" id="img_02" />


    <p class="after-cite">
      Douglas Engelbart voyait un potentiel dans les ordinateurs autre que celui de broyer des nombres. L’un des
      premiers usages qu’il suggère est celui d’un architecte concevant un bâtiment dans quelque chose de similaire à un
      logiciel de CAD<span class="margin-note"><em>Computer Aided Design</em> - Conception Assistée par
        Ordinateur</span>. Mais il est impensable de réaliser une telle tâche sur un superordinateur. Là où les
      ordinateurs
      de l’époque demandaient de préparer des bandes perforées, les nourrir à la machine et obtenir un résultat après
      calcul, l’application que souhaite Engelbart nécessite de créer des interfaces physiques permettant un retour
      immédiat de la machine. Le NLS possédera donc un clavier permettant de lui adresser directement des commandes. Il
      conçoit aussi quantité de prototypes pour arriver à son « Indicateur de position X-Y destiné à un système d’écran
      d’affichage ». Objet qui adoptera rapidement le surnom plus concis de « souris ». Malgré cette innovation
      fluidifiant le rapport entre humain et machine, l’ordinateur demande un temps d’apprentissage conséquent.
    </p>

    <p>
      Non satisfaits de la direction que prend le laboratoire de recherche dans les années qui suivent la démo de NLS,
      de nombreux chercheurs du SRI décident de rejoindre le Xerox Palo Alto Research Center, en emmenant avec eux
      l’idée de la souris. Au sein de PARC, des chercheurs tels que Alan Kay et Adele Goldberg travaillèrent sur le
      Xerox Alto, présenté en 1973. Il introduit le Smalltalk, à la fois environnement de développement graphique (tels
      les Environnements de Développements Intégrés que nous connaissons aujourd’hui) et gestionnaire de fenêtre.
      Smalltalk eut une grande influence sur la forme que prirent les langages de programmation orienté objet par la
      suite. Quand Kay et son équipe décident de le créer, ils souhaitent rendre le langage informatique naturel, de
      manière à ce que des néophytes puissent se l’approprier aisément. Dans un souci d’accessibilité, le Smalltalk
      introduit la métaphore du bureau.
    </p>

    <p>
      Le GUI de l’Alto pouvait alors posséder plusieurs applications ouvertes en même temps, dans différentes fenêtres.
      De plus cet ordinateur possède la première interface WYSIWYG (What You See Is What You Get) : cet acronyme désigne
      une nouvelle manière de travailler avec un ordinateur : le document sur lequel on travaille numériquement aura la
      même apparence dans le logiciel, qu’une fois imprimé. Cette logique rompt avec la traditionnelle compilation du
      code. Il n’est plus nécessaire de modifier un fichier et de le recompiler entièrement afin d’apprécier ses
      changements.
    </p>

    <p>
      Désormais, chaque entrée apparaît à l’utilisateur formatée, sous l’apparence que pourrait avoir le résultat final.
      Ce changement de paradigme permet aux utilisateurs de l’Alto d’itérer sur leur travail plus rapidement.
      L’utilisateur et l’ordinateur s’affectent mutuellement et réciproquement.</p>

    <p>
      La machine, dotée d’un écran vertical est pensée comme un outil de mise en page. Les ordinateurs de cette époque
      n’étant pas encore communément envisagés comme des supports de diffusion de médias ou de communication, la
      finalité de tout travail de mise en page se devait d’être imprimée. </p>

    <p>
      Les avancées que l’Alto aura introduites ne deviendront accessibles au public qu’en 1981. Là où l’Alto était un
      projet expérimental, le Xerox 8010 Information System, ou Xerox Star est un ordinateur commercial, destiné à être
      adopté par des bureaux. La métaphore du bureau introduit avec le Smalltalk se concrétise : il n’est plus
      nécessaire pour l’utilisateur d’être un spécialiste pour utiliser un ordinateur. Cependant, au vu du prix de
      l’installation d’un parc informatique (50 000 à 100 000 $ pour 2-3 ordinateurs, et serveurs), le Xerox Parc n’est
      vendu qu’à 25 000 exemplaires, c’est un échec commercial. Si nous connaissons assez peu les ordinateurs de Xerox
      Parc aujourd’hui, ce centre de recherche aura indirectement donné naissance au Lisa et au Macintosh, à la suite de
      la visite de Steve Jobs et d’ingénieurs d’Apple en 1979.
    </p>

    <p>
      Ces quelques exemples nous donnent une base pour comprendre l’origine de l’interface graphique. La volonté
      d’employer les ordinateurs pour de nouveaux usages devait passer nécessairement par la création d’une interface
      permettant l’entrée de données et un retour visuel en temps réel. Les périphériques de l’époque dédiés à cette
      tâche étant peu appropriés, de nouveaux furent créés, essentiellement par des ingénieurs. L’apparence que prirent
      ces outils de visualisations de données fut directement héritée d’un média préexistant : la page. Sans doute, cet
      héritage de la page est lié à la nature du travail à effectuer avec ces premiers ordinateurs : l’édition de
      documents. Les créateurs de ces interfaces revendiquèrent d’ailleurs leurs conceptions comme des simulations
      d’outils préexistants2, une reproduction virtuelle d’objets physiques auxquels on aurait ajouté de nouvelles
      propriétés. Les progrès informatiques de l’époque étaient motivés par la perspective d’employer les ordinateurs à
      des tâches nouvelles et d’émanciper technologiquement leurs utilisateurs : augmenter l’intellect humain. Peu de
      temps après les expérimentations pionnières de ces chercheurs, une entreprise vit dans l’interface graphique la
      possibilité d’offrir plus que des machines chargées de nouveaux usages. Apple souhaitait transformer l’ordinateur
      en un bien de consommation accessible à tous. </p>

    <p class="description">Page suivante : <em>Xerox Alto, 1973.</em></p>

      <img src="./img/chapter1/alto_4x.jpg" class="img-full-page" id="img_03" />

    </section>

  <section class="chapter" data-chapter="2">
    <h1 id="second">La simplicité se vend bien</h1>

    <p>Si aujourd’hui un terme résonne dans tous les discours des entrepreneurs de la Silicon Valley, c’est bien
      celui-ci : simplicité. À en voir les publicités vantant la facilité d’utilisation d’un téléphone ou d’une
      application, il semblerait que la moindre friction n’a pas sa place dans l’utilisation d’un logiciel. Dès lors que
      l’utilisateur viendrait s’interroger sur la manière d’utiliser le logiciel, l’interface n’aurait pas rempli son
      rôle. Tous les concepteurs semblent partager le même fantasme : celui d’une interface parfaitement instinctive, où
      le moindre doute serait inexistant.</p>

    <p>En 1984, Apple lance le Macintosh. Alors que l’Apple Lisa, prédécesseur du Macintosh était le premier ordinateur
      dont chaque fichier pouvait être représenté par une icône, Apple témoigne une fois de plus son intérêt pour le
      graphisme en informatique : Susan Kare fut la première designer graphique à être embauchée pour concevoir les
      icônes d’un ordinateur. Dans un secteur professionnel alors dominé par des ingénieurs, elle fut mandatée pour
      donner à l’ordinateur un aspect plus humain. La création d’un tel langage visuel annonçait la volonté d’Apple de
      rendre l’ordinateur accessible à un plus large public. Déjà, la recherche d’une forme d’utilisation instinctive
      effaçant la nécessité de connaissance à priori se fait sentir :</p>
    <p class="cite">« J'aime à penser que les bonnes icônes sont instantanément reconnaissables - même si quelqu'un ne
      l'a jamais vue, vous pouvez lui demander ce qu'elle fait, et ils comprennent - ou qu'il est si facile de s'en
      souvenir que si quelqu'un vous dit ce qu'elle est une fois, il est facile de s'en souvenir quand on la regarde. »
    </p>
    <p>Ce que le Macintosh laisse présager, c’est une tendance qui va influencer l’industrie du design d’interface
      jusqu’à nos jours. Il ne s’agit pas simplement de faire un ordinateur qui fonctionne. Il ne s’agit pas non plus de
      faire un ordinateur que des employés de bureau peuvent apprendre à utiliser. Il s’agit de faire un ordinateur
      simple.</p>
    <p>Ce terme reviendra de nombreuses fois lors de l’annonce d’un nouveau produit Apple en 2007. Steve Jobs révèle
      l’iPhone au grand public. L’écran recouvre désormais la quasi-totalité du hardware. Et si l’écran prend désormais
      tant de place (proportionnellement à la taille du dispositif), l’interface graphique est aussi au premier plan. La
      manière dont on utilise cet ordinateur est aussi bien différente des bureaux virtuels traditionnels. Parmi les
      changements notoires, la logique du système de fichier traditionnel des ordinateurs disparaît. Avec l’iPhone, plus
      d’explorateur de fichier. Des usages sont concentrés dans des applications, elles-mêmes représentées par une
      collection d’icônes colorées. Là où l’ordinateur traditionnel laissait accessibles les fichiers dans tout
      contexte, l’iPhone se montre fonctionnaliste et ne permet d’accéder à ses fichiers que dans le cadre d’un usage :
      écouter de la musique, naviguer dans sa galerie d’images... Au vu des dimensions de l’objet, plus question de
      rentrer de commandes textuelles. Si le terminal disparaît, c’est en fait toute la dimension programmable de
      l’ordinateur qui n’existe plus. L’iPhone est un ordinateur qui utilise des applications aux fonctions
      pré-programmées, pré-définies, mais qui ne permet pas de concevoir de nouvelles fonctions ou applications. C’est
      un ordinateur de consommation de services, de fonctions. En cela nous pouvons l’opposer aux machines dites
      complètes au sens de Turing. En simplifiant grandement, ce terme est dit d’une machine (ou encore d’un langage de
      programmation, d’une suite d’instructions) permettant d’effectuer n’importe quel calcul, et ainsi de se reproduire
      soit même. Ainsi s’il est théoriquement possible d’y faire tourner du code simulant une machine de Turing, ce
      potentiel de libre calcul est restreint par le fabricant qui ne permet à du code d’opérer uniquement après un
      passage par son App Store.</p>
    <p>Nous pouvons voir l’iPhone comme une concrétisation du design d’interface. Si le rôle de l’interface est
      effectivement d’éloigner l’utilisateur du code pour lui donner un catalogue d’interactions graphique, l’iPhone est
      l’objet qui en présente une version la plus aboutie lors de sa sortie. L’unique priorité de l’iPhone est d’être le
      plus affordant possible. En cela, cette version de 2007 proposera de nombreux éléments skeuomorphiques. Tenir un
      iPhone dans sa main, ce n’est plus vraiment tenir un ordinateur. Si l’on en croit les images que l’écran nous
      renvoie, on se situe tantôt devant une calculette dont les boutons se dessinent en relief, tantôt devant un
      bloc-note. L’apparence traduit la fonction de la machine, et non pas son fonctionnement. Ces métaphores faisant
      référence au monde réel furent privilégiées jusqu’à l’annonce d’iOS 7 ; où estimant que les utilisateurs étaient
      finalement assez familiers avec le fait de toucher du verre pour développer de nouveaux idiomes, un langage
      graphique propre à l’écran tactile fut développé.</p>
    <p>L’autre acteur majeur de la conception de système d’exploitation pour smartphones, est tout autant préoccupé par
      la facilité d’utilisation de ses produits. En 2014, Google annonçait son Material Design. Il est décrit comme « un
      système de directives, composants, et outils qui permet les meilleures pratiques du design d’interface. »</p>
    <p>Ces autoproclamées meilleures pratiques promettent la solution pour créer des interfaces « intuitives, et
      belles ». En mettant à disposition des API permettant aux développeurs de concevoir des applications rapidement
      tout en ayant une apparence correcte, Google instaure agressivement une forme unique aux interfaces des
      applications mobiles. La stéréotypisation des interfaces mobiles auxquelles nous assistons peut déjà soulever en
      nous des questions. N’est-il pas problématique que les utilisateurs de smartphones ne soient habitués qu’à une
      seule typologie d’interface ? Concevoir une interface qui serait basée sur des principes tout autres que ceux de
      Google Material en ferait un objet hors de la norme. En habituant des utilisateurs à une seule logique
      d’interface, on pourrait craindre que se développe une réticence à devoir se confronter à des formes inconnues. La
      démarche de Google dans la diffusion massive de son Material design est problématique dans le sens où il efface la
      possibilité d’une plus grande variété de formes. Au-delà de l’appauvrissement du paysage des possibles
      interactifs, il y a une sorte d’animosité envers ce qui ne s’intègre pas à l’écosystème Material. Pour Google, il
      n’y a pas la place pour d’autres formes que le Material design dans la grande convergence de l’expérience
      utilisateur.</p>
    <p>John Maeda, chercheur et designer, a fait l’éloge de Google Material au cours de la biennale internationale de
      Design de Saint-Étienne 2019. Dans son livre « De la simplicité », il nous présente un ensemble de règles visant à
      atteindre la simplicité, et plus spécifiquement en design. Cependant s’il présente son travail comme un moyen de
      remédier à la saturation du paysage informationnel, et ultimement comme un outil pour améliorer la vie des gens,
      ce n’est pas un travail désintéressé économiquement. Il l’annonce dès l’introduction de son livre :</p>
    <p class="cite">« En 2004, j’ai lancé au Media Lab, où je travaille, le MIT SIMPLICITY Consortium. Il rassemblait
      environ dix entreprises partenaires [...]. Notre mission consistait à définir comment, dans le domaine des
      communications, de la santé et des jeux, la simplicité peut avoir une valeur marchande. Ensemble nous avons
      imaginé et créé des systèmes et des technologies prototypes débouchant sur des produits soucieux de simplicité qui
      puissent être des réussites économiques. »</p>
    <p>Ainsi la simplicité ne serait pas uniquement un bien que l’on recherche pour l’utilisateur. Elle revêt une valeur
      pécuniaire. La simplicité rendrait la technologie accessible à tous. Mais bien loin de la volonté « d’augmenter
      l’intellect humain » qu’était celle de Engelbart ou Kay, la simplicité telle que recherchée pour les interfaces
      graphiques contemporaines aurait pour but de rendre accessible au plus de monde possible un bien de consommation.
    </p>
    <p>Si nous pouvons globalement considérer comme positif l’accès aux ordinateurs par le grand public, les motivations
      qui provoquent cette évolution proviennent plus d’un désir de rentabilité que d’un humanisme désintéressé. Je
      propose de voir l’interface graphique comme le prolongement du logiciel selon Anthony Masure :</p>
    <p class="cite">« Le logiciel partage avec le pinceau ou le marteau des pratiques codifiées et installées
      historiquement dans la culture. Toutefois, contrairement au pinceau ou au marteau, le logiciel exerce une forme de
      contrôle de par sa structure algorithmique. [...] Au contraire des outils analogiques, la présence de “langages
      formels” au sein des logiciels dit bien qu’il y a en eux de la “logique”. »</p>
    <p>Si les logiciels sont effectivement porteurs d’idéologies (et orientent les manières de faire), nous pouvons
      douter de la recherche systématique de « simplicité d’utilisation », et chercher à mettre en lumière les traits
      non désirables de cette philosophie axiomatique. Dès lors que nous acceptons que les interfaces ne soient pas que
      le résultat d’un désir pragmatique de faciliter la communication entre humain et ordinateur ; mais aussi le
      résultat de motivations économiques portées par des entreprises, nous pouvons porter une critique sur les logiques
      qu’elles renferment.</p>

  </section>
  <script src="https://unpkg.com/pagedjs/dist/paged.polyfill.js"></script>
  <script src="marginNotes.js"></script>

  <section class="chapter" data-chapter="3">
    <h1 id="third">Dissimuler</h1>
    <p>Anthony Masure propose une définition de l’interface comme « un empilement de couches techniques dont les effets
      masquent le fonctionnement interne des machines ». Il serait donc la nature même de l’interface de dissimuler le
      fonctionnement algorithmique de l’ordinateur, et d’éloigner l’utilisateur des entrailles de la machine. N’ayant
      pas accès au modèle technique réel du logiciel (son architecture de classes, fonctions, variables...), l’interface
      sert alors de support à l’utilisateur pour se construire sa conception mentale du fonctionnement de la machine.
    </p>
    <p class="cite break-cite">« En termes sémiotiques, l’interface informatique agit comme un code qui véhicule des
      messages
      culturels dans divers médias. [...] Dans la communication culturelle, il est rare qu’un code soit simplement un
      mécanisme de transport neutre ; il influe généralement sur les messages transmis par son intermédiaire. Il peut de
      la sorte faciliter l’expression de certains messages et en rendre d’autres inconcevables. Un code peut aussi
      apporter avec lui son propre modèle du monde, son propre système logique ou idéologique ; les messages culturels
      ou les langages entiers qu’il servira ensuite à créer seront limités par le modèle, le système ou l’idéologie qui
      l’accompagne. »</p>
    <p class="cite">« Cependant, les principes de la programmation informatique contemporaine, inconnus des
      utilisateurs, sont "projetés" au niveau de l'interface utilisateur, ce qui façonne la manière pratique dont les
      utilisateurs travaillent avec les applications logicielles de médias et dont ils comprennent cognitivement ce
      processus. »</p>
    <p>Ces deux citations de Lev Manovich mettent en évidence que l’interface graphique constitue une forme de barrière
      quant à la compréhension du fonctionnement d’un ordinateur. L’utilisateur ordinaire regarde son écran, mais ne
      pourra pas voir au-delà. Seulement, un ordinateur possède un comportement autonome. Il suffit d’ouvrir un
      gestionnaire de processus tout juste après le démarrage, pour constater qu’un ordinateur fait tourner plus d’une
      centaine de processus dissimulés. À l’heure où j’écris ces lignes, ayant pour seules « applications » ouvertes un
      éditeur de texte et un terminal affichant mes processus, l’ordinateur indique 123 tâches actives. Ces processus
      sont communément décrits comme « tournant en arrière-plan », soit derrière ce que l’utilisateur voit spontanément.
      Car la réalité de la grande partie d’utilisateurs s’arrête aux représentations que leur montrent leurs écrans,
      l’interface graphique. Nous pouvons d’ailleurs reprocher aux concepteurs de systèmes d’exploitation comme
      Microsoft ou Apple d’éloigner les utilisateurs de la réalité technique de l’ordinateur. Observons par exemple
      différentes manières suggérées d’installer un logiciel sur différents systèmes d’exploitation.</p>
    <p>Dans l’exemple présenté sous OSX, l’installation est réalisée par un geste plus symbolique qu’autre chose :
      traîner l’icône du logiciel dans le dossier « applications ». Aucune autre information n’est présente, que ce soit
      le poids qu’occupera le logiciel sur l’ordinateur, la destination réelle des fichiers. Ce manque d’informations
      sur les processus à l’œuvre lors de l’installation d’un logiciel peut être représentatif de ce que les concepteurs
      de ces interfaces pensent de l’utilisateur standard : soit trop stupide pour comprendre des informations obscures,
      soit désintéressé de savoir ce qui ce passe dans son ordinateur. Nous pouvons regretter qu’ils ne profitent pas de
      ce moment où l’utilisateur vient affecter son système d’exploitation pour rendre intelligible la structure de
      fichier de ce dernier. L’interface reprendrait un rôle de médiatrice, venant faciliter la tâche, mais aussi
      éduquer, en représentant l’information de manière digeste plutôt qu’en l’occultant.</p>
    <p>Si l’installation d’un logiciel sous Windows nous apporte une quantité d’information plus riche que pour OSX,
      cela reste moins spécifique (même si certes, moins austère) qu’une installation à la ligne de commande sur un
      système d’exploitation de type Linux. Dans le cas de cette méthode spécifique, chaque paquet, que l’on peut
      considérer comme des fragments autonomes du logiciel sont détaillés. L’utilisateur est donc libre d’aller
      consulter la fonction spécifique de chaque élément qui compose le logiciel. Sans pour autant affirmer que cette
      surcharge d’information à un quelconque intérêt pragmatique lorsqu’on souhaite juste installer un logiciel, cela
      procure à l’utilisateur un aperçu concret de la composition de son logiciel.</p>
    <p>Ces représentations ne sont bien sûr pas exhaustives : de nos jours, nous pouvons trouver sur chacun de ces
      systèmes d’exploitation un magasin d’application permettant d’installer une vaste collection de logiciels en un
      clic de souris. Les manières décrites ci-dessus sont cependant toujours d’usage et largement répandues.</p>
    <p>Sans dévoiler à l’utilisateur comment fonctionne son ordinateur ou sans l’encourager à aller plus loin dans la
      compréhension de sa machine ; l’utilisateur forme le « plan mental » de son ordinateur uniquement à partir de
      l’interface. Cette intelligence fonctionnelle1 est problématique, car dès lors que les symboles de l’interface
      changeront, l’utilisateur devra passer par une nouvelle phase d’apprentissage. Nous devons nous opposer aux
      interfaces qui impliquent une forme d’apprentissage uniquement contextuelle, empêchant de réemployer ses
      connaissances dans d’autres environnements logiciels. Car le langage des interfaces pourrait effectivement
      s’appuyer sur des bases présentes dans tous les logiciels : classes, fonctions, variables... En somme, la création
      d’une interface ayant vocation à donner un savoir de fond à l’utilisateur doit reposer sur une représentation
      intelligible du modèle technique réel du logiciel.</p>
    <p>Si l’on nous reproche que rattacher les actions interfacées au fonctionnement algorithmique résulte en des
      interfaces de spécialiste, il convient de ne pas oublier la nature dynamique de l’informatique. La quantité
      d’information à soumettre à l’utilisateur peut être modulée, et nous pouvons aisément imaginer des strates
      d’informations. Prenons l’exemple d’un utilisateur utilisant un logiciel de retouche photo. En cherchant à
      comprendre le mode d’un fonctionnement d’un outil, il se référerait à une section d’aide à l’écran et pourrait
      choisir le niveau d’exactitude d’explication. La première explication serait fonctionnelle, les strates
      intermédiaires seraient des explications du modèle conceptuel de la fonction allant du plus simple au plus précis,
      et enfin viendrait l’accès au code. Dans une dynamique moins portée sur l’enseignement que la facilité d’accéder
      au code du logiciel, l’exemple d’Inkscape est intéressant. Logiciel de dessin vectoriel, il propose un éditeur XML
      afin de voir quel code est produit lors du tracé d’une courbe ou de toute autre forme géométrique. Il est aussi
      possible d’écrire directement dans cet éditeur pour générer des formes. En permettant l’accès à de telles données,
      l’utilisateur peut non seulement augmenter sa compréhension de la manière dont un ordinateur interprète la
      représentation d’images vectorielles, mais aussi avoir un outil complémentaire à ceux purement graphiques de
      l’interface.</p>
    <p>Il ne s’agit pas ici de revendiquer un design d’interface ayant « une transparence totale » quant au
      fonctionnement de l’ordinateur. Plutôt, je souhaite pointer du doigt les interfaces qui forment une barrière
      infranchissable, tant psychologique que technique. Je ne pense pas que dissimuler la réalité technique d’un
      ordinateur pour en faciliter son accès soit une fin en soi. Elles devraient plutôt être pensées comme une étape
      d’un chemin vers une compréhension plus profonde des ordinateurs, et tendre à créer des <em>powerusers</em>, et
      non pas des utilisateurs consommateurs de fonctions pré-encodées.</p>
  </section>

  <section class="chapter" data-chapter="4">
    <h1 id="fourth">Déterminer</h1>
    <p>Alexander Galloway désigne les interfaces comme des « zones d’activités autonomes », se déviant de la vision
      traditionnelle d’un objet physique ou virtuel. Il souligne de cette manière des processus qui émergeraient hors de
      la volonté de l’utilisateur lors de leur manipulation. Pour lui, il ne s’agit pas d’étudier les interfaces en tant
      qu’objets individuels, mais plutôt selon les effets qu’elles produisent. Je souhaite plus particulièrement montrer
      en quoi elles entraînent un comportement spécifique de la part d’un utilisateur.</p>
    <p>Les interfaces sont des produits d’origines historiques et sociales. Le catalogue de fonctions proposé, ainsi que
      la manière dont ces fonctions sont encodées, est influencé par une culture de l’équipe de développement. Prenons
      l’exemple concret des logiciels de retouche photo. Photoshop possède des fonctions grandement automatisées pour
      effectuer des tâches spécifiques : estomper des tâches, tendant à faciliter grandement la disparition de ce qui
      n’est pas complètement lisse, ce qui se détache des aplats de couleurs immaculés. On peut y voir ici l’influence
      de la culture de l’équipe de développement qui fournit des fonctionnalités dont le but est la productivité, et
      facilitent la retouche des images qui correspondent à un certain standard de la photographie (telle que l’image
      que nous pourrions avoir de la photographie de publicité, de mode). Nous pouvons aussi considérer un exemple
      peut-être plus stéréotypé encore dans le domaine de la photographie. L’application de photographie du système
      d’exploitation MIUI possède une fonctionnalité « beauté » permettant de modifier l’image lors de la prise de
      selfie sur différents traits. L’application révèle alors explicitement les critères de beauté des développeurs :
      grands yeux, teint clair, peau lisse, mâchoire fine. L’intensité de chacun de ces paramètres peut d’ailleurs être
      réglée individuellement.</p>
    <p>Ainsi la forme des fonctions à la disposition des utilisateurs n’est pas neutre, mais le produit de choix de
      l’équipe de développement : choix influencés par un ensemble de paramètres idéologiques, politiques, culturels. Il
      convient de se rappeler que ces fonctions définissent la marge d’expression d’un utilisateur. Les logiciels
      possèdent une détermination technique, en le sens qu’un utilisateur ne pourra toujours qu’agir dans les limites
      que lui permet l’interface. Cette détermination technique n’est cependant pas systématiquement perçue, par
      l’utilisateur ou même le concepteur de cette interface.</p>
    <p>Ces dernières années les outils de conceptions « sans code » ont grandement pris en popularité. Si ces nouvelles
      plateformes permettent à un tout nouveau public de concevoir sites web et applications mobiles avec une courbe
      d’apprentissage moindre, elles ne sont pas sans inconvénient. Ces applications ne pouvant offrir la même liberté
      que la programmation, elles tendent à fournir un environnement visant la productivité, plutôt que la recherche de
      formes nouvelles. Les formes qui en sont issues tendent à être stéréotypées, et malgré le sentiment de création
      que de tels objets peuvent fournir, il s’agit surtout de l’emploi d’une solution, plus que d’un outil permettant
      de créer.</p>
    <p class="cite">« La simplicité se voit aujourd’hui mise au service du principe d’efficacité, paradigme dominant de
      notre société, afin que l’utilisateur ne perde pas de temps et exécute rapidement ce qu’il souhaite à travers
      l’interface. Cet axiome est ainsi devenu une loi d’airain du web design, formalisée par Steve Krug dans son livre
      Don’t make me think : “je devrais être capable de comprendre ce que c’est et comment l’utiliser sans aucun effort
      pour y penser”. Ces lois, axiomes ou principes sont souvent considérés comme indiscutables et imprègnent les
      bonnes pratiques des professionnels. »</p>
    <p>Si les outils de conceptions sans codes sont développés par des entreprises avec le désir d’être manipulable sans
      connaissances préalables, il est paradoxal d’imaginer pouvoir « créer » en ne manipulant qu’un assemblage
      d’élément. Si nous prenons l’exemple des solutions logicielles pour créer des sites web sans code, elles proposent
      généralement aux utilisateurs une sélection de <em>templates</em>. L’utilisateur va alors remplacer texte et
      images, éventuellement modifier quelques couleurs où typographies, mais le résultat sera toujours dans le cadre
      que lui confère l’interface. Les sites web issus de ces solutions correspondront donc à un standard. Ce problème
      se retrouve aussi lorsque des développeurs emploient des librairies leur fournissant des éléments d’UI déjà prêts
      (type bootstrap) : nous perdons une certaine richesse de variété visuelle dans le paysage web au profit d’une
      efficacité de développement. En plus de l’uniformisation du paysage graphique de l’internet, nous pouvons
      critiquer le manque de recul que ce type de solutions logicielles provoque. En « augmentant le niveau
      d’interface » d’un logiciel (entendre par là, éloigner l’utilisateur de la nécessité de programmer son travail),
      on produit une forme d’automatisation. Tout ce qui n’est pas à coder est généré « automatiquement », en un
      processus qui existe en dehors de la compréhension de l’utilisateur. Nous pouvons alors rapprocher la
      concrétisation des interfaces graphiques au processus d’automatisation de la prise de décision chez Bernard
      Stiegler. Si l’automatisation de la correction des fautes d’orthographe ainsi que la suggestion de formulations
      sur un logiciel de traitement de texte mènent à une dégradation du niveau d’orthographe des utilisateurs ainsi
      qu’une perte de variété du langage, nous pouvons craindre que l’automatisation de l’action de l’utilisateur lors
      de la création d’un site internet par un outil « no code » ne mène là aussi à une perte d’autonomie de
      l’utilisateur, et une atrophie de sa compréhension de la création de sites internet.</p>
    <p>Lev Manovich propose une vision de l’utilisation d’un logiciel comme extrêmement déterminante, où le logiciel
      n’est pas juste un outil qui va influencer la forme d’un geste.</p>
    <p class="cite">« On affirme souvent que l’utilisateur d’un programme interactif arborescent en devient le
      coauteur : en choisissant un chemin unique à travers les éléments d’une œuvre, il en créerait une nouvelle. Mais
      on peut aussi envisager ce processus autrement. Si une œuvre achevée est la somme de tous les chemins possibles à
      travers les éléments, alors l’utilisateur qui n’en suit qu’un n’accède qu’à une partie du tout. Autrement dit, il
      n’active qu’une partie d’une œuvre qui existe déjà en totalité. »</p>
    <p>Si l’utilisation d’un logiciel tel qu’il a été pensé nous condamne à ne pouvoir que retrouver des résultats qui
      préexistent à l’emploi de ce logiciel, il est nécessaire de s’éloigner de l’emploi traditionnel de ces solutions
      pour véritablement « créer ». Ou bien, il s’agirait pour les développeurs de mettre en évidence la tendance de
      leurs logiciels à influencer le comportement :</p>
    <p class="cite">« Il s’agit ici de penser une méthode d’approche qui s’efforcerait de faire paraître les
      “déterminations techniques” des logiciels. [...] Donner à voir les déterminations logicielles c’est précisément
      les comprendre et échapper consciemment à leurs orientations. Faire paraître l’en-dehors du programme conteste la
      reproduction servile d’un usage attendu. »</p>
    <p>Là où les exemples précédents n’ont pas un impact dramatique sur les utilisateurs (si ce n’est faussement
      procurer un sentiment de liberté), l’influence des interfaces est dans certains contextes mise à profit par des
      développeurs. Les darks patterns désignent des procédés de design d’interfaces visant à obtenir d’un utilisateur
      un comportement qu’il n’aurait pas spontanément : s’abonner à une chaîne de mail, accepter des cookies, acheter un
      produit... Ces méthodes abusent de biais cognitifs pour obtenir le résultat voulu, et sont le fruit de recherches
      en science comportementale.</p>
    <p>Seulement, si la manipulation consciente des développeurs est aussi répandue dans un objectif pécuniaire, ne
      peut-on pas imaginer des good patterns, dont le but serait d’émanciper l’utilisateur de l’influence des
      interfaces ? On rejoint certainement les idées d’Anthony Masure en suggérant de rendre visible l’influence des
      interfaces, mais il s’agit ici aussi de donner des clés de compréhension à l’utilisateur sans qu’il s’en rende
      nécessairement compte. Après tout, certains jeux vidéo utilisent déjà de tels stratagèmes afin d’inculquer les
      règles qui régissent le jeu sans tutoriel explicite. Je ne suggère pas par-là de <em>gamifier</em> l’apprentissage
      de l’informatique, mais plutôt de provoquer des situations ou l’utilisateur développerait une compréhension de la
      programmation sous forme de réflexe plutôt que de manière consciente.
    </p>
  </section>

  <section class="chapter" data-chapter="5">
    <h1 id="fifth">Une sociéte scindée</h1>

    <p>Nous vivons dans une « société du logiciel ». Si tout le monde possède aujourd’hui un concentré de puissance à
      calculer dans sa poche, bien peu sont ceux qui peuvent prétendre comprendre comment ces machines fonctionnent.
      Nous pouvons alors constater une scission au sein de la société. D’un côté nous avons les utilisateurs ordinaires.
      Sachant utiliser téléphones et ordinateurs dans un contexte de tâches déterminées : consulter ses mails, acheter
      des objets en ligne, utiliser une suite bureautique. De l’autre nous avons des concepteurs. Ils constituent un
      groupe d’individus bien plus réduit que le premier, mais c’est eux qui pensent, et conçoivent logiciels et
      interfaces. Ils savent utiliser les ordinateurs, non pas seulement pour effectuer des tâches pré-programmées, mais
      ils savent aussi manipuler l’objet afin d’y ajouter de nouvelles fonctions, d’en modifier des préexistantes. Ils
      ont une compréhension structurelle de l’ordinateur. J’inclus donc dans ce groupe les designers d’interfaces.</p>
    <p>La séparation manichéenne que je marque ici n’est pas sans rappeler celles qu’Abraham Moles fait quand il parle
      « des Hommes de la Fonction » les clients, consommateurs de biens matériels, et « les Hommes de la Structure » :
      constructeurs, inventeurs, réparateurs. Un parallèle évident se dresse entre l’interface graphique et la
      description qu’il fait du « carter », cette boite esthétique qui vient recouvrir l’intérieur de l’objet,
      dissimulant ses organes internes et venant occulter la complexité technique de l’objet pour suggérer une manière
      adaptée de l’utiliser. « Sa tâche est d’adapter le ‘c’est fait de’ au ‘c’est fait pour’ ».</p>
    <p class="cite break-cite">« Son [l’utilisateur] intelligence de l’objet, c’est une intelligence fonctionnelle.
      C’est sa
      capacité à bien s’en servir. Mais si l’objet est ‘en panne’ [...], la complexité de sa structure fait irruption
      dans le flux vital comme un obstacle, comme un incident, comme un accident. [...] Insistons : le carter, cette
      enveloppe plus ou moins résistante aux chocs, couvrant la machine, l’appareil, est une frontière tout autant
      psychologique, juridique, symbolique, qu’utilitaire. Pour tous les outils un peu complexes : sèche-cheveux,
      téléviseurs ou amplificateurs Hi-Fi, le carter reste scellé juridiquement inviolable par l’utilisateur. Ainsi le
      propriétaire (?) possède l’extérieur, mais, en fait, pas l’intérieur : il l’ignore, il n’y a pas accès. »</p>
    <p class="after-cite">Nous pouvons alors pointer du doigt un autre problème inhérent à l’interface graphique. Elle
      aussi constitue
      souvent une frontière juridique. Car dans le cas d’un logiciel propriétaire, le code du logiciel est complètement
      inconnu à l’utilisateur. Impossible de savoir ce qui se passe derrière ces fenêtres, menus déroulants et liste de
      fonctions. L’utilisateur n’a pas le droit (j’insiste, au sens juridique) de lire ce code, de le modifier.
      Cette frontière est aussi symbolique, et psychologique. Dresser une interface qui ne peut être franchie entre un
      individu et le code du logiciel, c’est aussi réserver une forme de privilège à ceux qui conçoivent ces logiciels.
      Je ne revendique pas que tout le monde puisse, ou souhaite accéder au code des logiciels. Mais en privilégiant
      affordances d’une interface qui se suffit à elle-même et en verrouillant le code, on prive une partie de la
      population qui pourrait y avoir accès.</p>
    <p>Dans la continuité de la réflexion de Anthony Masure, nous pouvons craindre que l’interface, prolongement du
      logiciel mène à une « perte de savoir-faire ».</p>
    <p class="cite break-cite">« Le logiciel « encode » des savoir-faire qui, dès lors, cessent de faire l’objet de
      connaissances
      et de transmissions. Des métiers sont déplacés, transformés, supprimés. On observe des tensions entre les
      anciennes professions et les nouvelles. Karl Marx fait de la machine ce qui retire (fait oublier) à l’ouvrier le
      geste traditionnel pour en faire un simple exécutant, un « prolétaire ». De la même façon, Lev Manovich observe
      dans les logiciels ce qui fait du savoir-faire un code, c’est-à-dire une entité calculable. Économiser des
      savoir-faire signifie qu’on les écarte d’une libre circulation et transmission. L’économie des logiciels est ce
      qui permet une simplification des savoirs (sans transmission, j’ai l’impression d’avoir accès à des compétences),
      mais aussi ce qui crée de l’oubli et du mal-être.
      L’encodage ne signifie pas que je maîtrise le sens de ce que je fais, car ce savoir-faire est désormais
      intransmissible (au sens où enseigner, c’est comprendre le sens de ce qu’on enseigne). Parallèlement,
      l’extériorisation de compétences dans des logiciels est problématique quand elle les soumet à une potentielle
      captation. Il n’est plus possible de disposer librement de savoir-faire, puisque leur extériorisation technique,
      consubstantielle
      à l’existence humaine, est soumise à un rendement discriminant. »</p>
    <p class="after-cite">Ce qui est pointé du doigt ici, c’est que le « savoir-faire », une forme de connaissance
      profonde qui devrait
      pouvoir être transmise, utilisée dans différent contexte, une fois encodée dans un logiciel est remplacée par une
      forme de connaissance contextuelle. L’utilisation d’un logiciel ne nous mène pas tant à développer des pratiques
      que l’on puisse réemployer dans d’autres typologies d’interface, que d’assimiler les symboles ou noms de fonctions
      d’une interface graphique à un résultat sur notre écran. Cette perte de connaissance est d’autant plus aliénante
      qu’elle nous enferme dans des écosystèmes visuels dont se séparer nous demande un effort significatif.</p>
    <p>Ces écosystèmes visuels sont largement influencés par un petit groupe puissant d’acteurs technologiques. Les
      standards conçus par ses entreprises sont ensuite repris par d’autres designers d’interfaces soit faute de moyens
      techniques (pourquoi développer une esthétique singulière ou des outils spécifiques si on met gratuitement à ma
      disposition un framework me permettant d’avoir des interfaces à l’apparence correcte ?), soit faute de moyens
      intellectuels (pourquoi proposer autre chose que ce système affordant ? Utiliser les pratiques existantes, c’est
      s’inscrire dans une norme qui est familière aux utilisateurs).
      Si l’influence de ces acteurs ne contraint pas à créer une interface selon leurs règles, ne pas les suivre, c’est
      développer un objet marginal, susceptible de déstabiliser un potentiel utilisateur.</p>
    <p>Il est problématique de laisser seul ce petit groupe d’acteurs puissant laisser influencer à ce point le paysage
      du design d’interface. À l’heure où les scandales liés aux abus de ces groupes se multiplient (surveillance
      intrusive, monétisations des données, verrouillages matériels et logiciels empêchant la possibilité de réparation
      soi-même, manipulation de masse, recul démocratique), ne devons-nous pas repenser la nature de nos relations avec
      les ordinateurs ?</p>
    <p>Alors, comment s’opposer à cette division aliénante pour une partie de la population ? Gilbert Simondon esquisse
      dans son livre Du mode d’existence des objets techniques la nécessité de développer une relation sociale avec les
      objets techniques. Il dresse le portrait de l’organisation industrielle de son époque, où des ingénieurs
      conçoivent machines et structures d’usines sans y être confrontés et des ouvriers aliénés par la manipulation de
      machines dont ils n’ont qu’une compréhension fonctionnelle. Il propose alors une sorte de retour à l’artisanat
      assisté par machine, afin d’effacer ce rapport de domination. L’homme devrait entretenir « une relation d’égalité,
      de réciprocité d’échanges : une relation sociale [avec l’objet technique]. ». L’individu qui opère la machine
      serait alors en mesure de la modifier, la régler, et cette dernière est pensée pour être sensible à ce type de
      variations.</p>
    <p>Appliquer cette utopie au domaine de l’informatique serait encourager les utilisateurs à se plonger au-delà de ce
      que leur présente l’interface graphique, et développer des outils de compréhensions des ordinateurs. Alors nous
      pourrons envisager être souverains de nos ordinateurs, comprendre les processus à l’œuvre dans nos machines, et
      développer lors de la manipulation des ordinateurs des connaissances de profondeur, réemployables dans toute
      typologie d’interface.</p>

  </section>

  <section class="chapter" data-chapter="6">
    <h1 id="sixth">Pour conclure</h1>
  </section>

  <section class="biblio">
    <h3 id ="seventh">Bibliographie</h1>
      <ul class="biblio-list">
        <li>Baricco, Alessandro, et Vincent Raynaud. 2019. <i>The game: essai</i>.</li>
        <li>CNIL. s.&nbsp;d. <i>La forme des choix</i>. Cahiers Innovations et Prospectives 6.</li>
        <li>Fuller, Matthew. 2003. <i>Behind the Blip: Essays on the Culture of Software</i>. Brooklyn, NY: Autonomedia.</li>
        <li>Galloway, Alexander R. 2012. <i>The interface effect</i>. Cambridge, UK ; Malden, MA: Polity.</li>
        <li>Krug, Steve. 2012. <i>Don’t Make Me Think! A Common Sense Approach to Web Usability.</i>Pearson.</li>
        <li>Lartigaud, David-Olivier, éd. 2017. <i>Objectiver</i>. Saint-Etienne: EPCC Cité du design, École supérieure d’art et design.</li>
        <li>Maeda, John, et Jean-Luc Fidel. 2009. <i>De la simplicité</i>. Paris: Éd. Payot &amp; Rivages.</li>
        <li>Manovich, Lev. 2013. <i>Software takes command: extending the language of new media</i>. International texts in critical media aesthetics. New York ; London: Bloomsbury.</li>
        <li>Manovitch, Lev, et Richard Crevier. 2015. <i>Le langage des nouveaux médias</i>.</li>
        <li>Masure, Anthony. 2014. «&nbsp;Le design des programmes, des façons de faire du numérique&nbsp;». Paris 1 Panthéon-Sorbonne.</li>
        <li>Simondon, Gilbert, et Nathalie Simondon. 2012. <i>Du mode d’existence des objets techniques</i>. Nouv. éd. rev. et Corr. Philosophie. Paris: Aubier.</li>
        <li>Thomas, Clément, Thomas Riollet, et Arthur Doublet Susini. s.&nbsp;d. <i>De l’influence des interfaces, entretien avec Anthony Masure</i>. Emilieu Studio.</li>
        <li>Triclot, Mathieu. 2017. <i>Philosophie des jeux vidéo</i>.</li>
        <li>Wurster, Christian. 2002. <i>Computers: an illustrated history</i>. Koln: Taschen.</li>
      </ul>

    <h3>Sitographie</h3>
    <ul class="biblio-list">
      <li>Alexander Bus. 2019. <i>Jonathan Blow - Preventing the Collapse of Civilization (English only)</i>. <a href="https://www.youtube.com/watch?v=pW-SOdj4Kkk">https://www.youtube.com/watch?v=pW-SOdj4Kkk</a>.</li>
      <li>«&nbsp;Apple - iPhone - Features&nbsp;». s.&nbsp;d. Consulté le 1 février 2021. <a href="https://web.archive.org/web/20071006005308/http://www.apple.com/iphone/features/index.html">https://web.archive.org/web/20071006005308/http://www.apple.com/iphone/features/index.html</a>.</li>
      <li>«&nbsp;Bernard Stiegler « Nous devons rendre aux gens le temps gagné par l’automatisation »&nbsp;». 2016. L’Humanité. 17 juin 2016. <a href="https://www.humanite.fr/bernard-stiegler-nous-devons-rendre-aux-gens-le-temps-gagne-par-lautomatisation-609824">https://www.humanite.fr/bernard-stiegler-nous-devons-rendre-aux-gens-le-temps-gagne-par-lautomatisation-609824</a>.</li>
      <li>Brygo, Julien. 2020. «&nbsp;Travail, famille, Wi-Fi&nbsp;». Le Monde diplomatique. 1 juin 2020. <a href="https://www.monde-diplomatique.fr/2020/06/BRYGO/61870">https://www.monde-diplomatique.fr/2020/06/BRYGO/61870</a>.</li>
      <li>«&nbsp;computability - Is a device with restrictive execution policies Turing-complete?&nbsp;» s.&nbsp;d. Computer Science Stack Exchange. Consulté le 8 janvier 2021. <a href="https://cs.stackexchange.com/questions/2243/is-a-device-with-restrictive-execution-policies-turing-complete">https://cs.stackexchange.com/questions/2243/is-a-device-with-restrictive-execution-policies-turing-complete</a>.</li>
      <li>«&nbsp;Design&nbsp;». s.&nbsp;d. Material Design. Consulté le 8 janvier 2021. <a href="https://material.io/design">https://material.io/design</a>.</li>
      <li>Game Maker’s Toolkit. 2015. <i>Half-Life 2’s Invisible Tutorial | Teaching Players</i>. <a href="https://www.youtube.com/watch?v=MMggqenxuZc">https://www.youtube.com/watch?v=MMggqenxuZc</a>.</li>
      <li>InriaChannel. 2014. <i>La société automatique, par Bernard Stiegler</i>. <a href="https://www.youtube.com/watch?v=999kzydPHGg">https://www.youtube.com/watch?v=999kzydPHGg</a>.</li>
      <li>«&nbsp;La voie néguentropique - Ép. 4/5 - Bernard Stiegler, la philosophie et la vie&nbsp;». s.&nbsp;d. France Culture. Consulté le 28 décembre 2020. <a href="https://www.franceculture.fr/emissions/a-voix-nue/bernard-stiegler-45-la-voie-neguentropique">https://www.franceculture.fr/emissions/a-voix-nue/bernard-stiegler-45-la-voie-neguentropique</a>.</li>
      <li>«&nbsp;Qu’est-ce que Turing Complete? IP Girl&nbsp;». s.&nbsp;d. Consulté le 8 janvier 2021. <a href="https://www.ipgirl.com/2710/quest-ce-que-turing-complete.html">https://www.ipgirl.com/2710/quest-ce-que-turing-complete.html</a>.</li>
      <li>«&nbsp;Susan Kare on Working on the Macintosh&nbsp;». s.&nbsp;d. Consulté le 15 janvier 2021. <a href="https://web.stanford.edu/dept/SUL/library/mac/primary/interviews/kare/mac.html">https://web.stanford.edu/dept/SUL/library/mac/primary/interviews/kare/mac.html</a>.</li>
      <li>>«&nbsp;The No-Code Generation is arriving | TechCrunch&nbsp;». s.&nbsp;d. Consulté le 8 janvier 2021. <a href="https://techcrunch.com/2020/10/26/the-no-code-generation-is-arriving/">https://techcrunch.com/2020/10/26/the-no-code-generation-is-arriving/</a>.</li>
      <li>Université Jean Moulin Lyon 3. 2014. <i>Vivre par(mi) les écrans - Session 3-De, par et à travers&nbsp; Immersion et interface- Bernard Stiegler</i>. <a href="https://www.youtube.com/watch?v=ALBre0fAT-E&amp;t=3073s">https://www.youtube.com/watch?v=ALBre0fAT-E&amp;t=3073s</a>.</li>
      <li>Université Sorbonne Paris Cité. 2017. <i>Rencontre avec Alain Damasio - Festival des idées Paris / USPC</i>. <a href="https://www.youtube.com/watch?v=_CKyrAK4TIw">https://www.youtube.com/watch?v=_CKyrAK4TIw</a>.</li>
    </ul>

    <h3>Iconographie</h3>
  </section>

  <section class="merci">
    <h3>Remerciements</h3>
  </section>

  <section class="acheve">
    <p id="mentions">Achevé d'imprimer à l'ESADSE <br> en février 2021</p>
  </section>
</body>

</html>